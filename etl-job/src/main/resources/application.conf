# Application configuration
application {
  name = "Chess Games ETL"
  environment = "local"
}

# Spark configuration
spark {
  master = "local[*]"

  # Spark application configurations
  conf {
    "spark.sql.shuffle.partitions" = 4
    "spark.default.parallelism" = 4
    "spark.sql.files.maxPartitionBytes" = "128MB"
  }
}

# File paths configuration
paths {
  input {
    base-path = "src/main/resources/input"
    chess-games = ${paths.input.base-path}"/sample_data.csv"
  }

  output {
    base-path = "src/main/resources/output"
    processed-games = ${paths.output.base-path}"/processed_chess_games"
  }
}

# Data processing configuration
processing {
  batch-size = 10000
  delimiter = ","
  write-mode = "overwrite"

  # Schema configurations
  schema {
    date-format = "yyyy-MM-dd HH:mm:ss"
    timestamp-columns = [
      "created_at",
      "last_move_at"
    ]
  }
}

# Logging configuration
logging {
  level = "INFO"
  spark-log-level = "WARN"
}
